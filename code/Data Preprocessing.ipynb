{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6116155,"sourceType":"datasetVersion","datasetId":2852448}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:21:54.747984Z","iopub.execute_input":"2025-03-08T23:21:54.748794Z","iopub.status.idle":"2025-03-08T23:22:03.313500Z","shell.execute_reply.started":"2025-03-08T23:21:54.748767Z","shell.execute_reply":"2025-03-08T23:22:03.312381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport mediapipe as mp\nimport matplotlib.pyplot as plt\n\ndef detect_and_save_hand(image_path, output_path, img_size=400, padding=50):\n    mp_hands = mp.solutions.hands\n    mp_drawing = mp.solutions.drawing_utils\n    mp_drawing_styles = mp.solutions.drawing_styles\n    \n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:\n        results = hands.process(image_rgb)\n        \n        if not results.multi_hand_landmarks:\n            print(\"No hands detected in\", image_path)\n            return\n        \n        landmarks = results.multi_hand_landmarks[0]\n        h, w, _ = image.shape\n        \n        # Extract landmark coordinates\n        points = [(int(l.x * w), int(l.y * h)) for l in landmarks.landmark]\n        \n        # Get bounding box of hand\n        x_min, y_min = np.min(points, axis=0)\n        x_max, y_max = np.max(points, axis=0)\n        \n        # Expand bounding box\n        x_min, y_min = max(0, x_min - padding), max(0, y_min - padding)\n        x_max, y_max = min(w, x_max + padding), min(h, y_max + padding)\n        \n        # Create a blank white image\n        hand_img = np.ones((img_size, img_size, 3), dtype=np.uint8) * 255\n        \n        # Scale and center hand skeleton\n        hand_width, hand_height = x_max - x_min, y_max - y_min\n        scale = (img_size - 2 * padding) / max(hand_width, hand_height)\n        offset_x = (img_size - hand_width * scale) // 2\n        offset_y = (img_size - hand_height * scale) // 2\n        \n        scaled_points = [\n            (int((x - x_min) * scale + offset_x), int((y - y_min) * scale + offset_y))\n            for x, y in points\n        ]\n        \n        # Draw hand skeleton\n        for connection in mp_hands.HAND_CONNECTIONS:\n            x1, y1 = scaled_points[connection[0]]\n            x2, y2 = scaled_points[connection[1]]\n            cv2.line(hand_img, (x1, y1), (x2, y2), (0, 0, 0), 2)\n        \n        # Draw hand landmarks\n        for x, y in scaled_points:\n            cv2.circle(hand_img, (x, y), 2, (0, 0, 255), -1)\n        \n        cv2.imwrite(output_path, hand_img)\n\n# Example usage\ndetect_and_save_hand(\"/kaggle/input/rgb-arabic-alphabets-sign-language-dataset/RGB ArSL dataset/Ghain/Ghain_0.jpg\", \"output.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:22:03.315547Z","iopub.execute_input":"2025-03-08T23:22:03.315829Z","iopub.status.idle":"2025-03-08T23:22:03.606193Z","shell.execute_reply.started":"2025-03-08T23:22:03.315804Z","shell.execute_reply":"2025-03-08T23:22:03.605220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport cv2\nimport mediapipe as mp\nfrom tqdm import tqdm\nimport sys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:22:03.607486Z","iopub.execute_input":"2025-03-08T23:22:03.607881Z","iopub.status.idle":"2025-03-08T23:22:03.612898Z","shell.execute_reply.started":"2025-03-08T23:22:03.607801Z","shell.execute_reply":"2025-03-08T23:22:03.611944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folders = [\n    \"Alef\", \"Beh\", \"Teh\", \"Theh\", \"Jeem\", \"Hah\", \"Khah\", \"Dal\", \n    \"Thal\", \"Reh\", \"Zain\", \"Seen\", \"Sheen\", \"Sad\", \"Dad\", \"Tah\", \n    \"Zah\", \"Ain\", \"Ghain\", \"Feh\", \"Qaf\", \"Kaf\", \"Lam\", \"Meem\", \n    \"Noon\", \"Heh\", \"Waw\", \"Yeh\", \"Laa\", \"Teh_Marbuta\", \"Al\"\n]\n\nEnAr = {\n    \"Alef\": \"أ\", \"Beh\": \"ب\", \"Teh\": \"ت\", \"Theh\": \"ث\",\n    \"Jeem\": \"ج\", \"Hah\": \"ح\", \"Khah\": \"خ\", \"Dal\": \"د\",\n    \"Thal\": \"ذ\", \"Reh\": \"ر\", \"Zain\": \"ز\", \"Seen\": \"س\",\n    \"Sheen\": \"ش\", \"Sad\": \"ص\", \"Dad\": \"ض\", \"Tah\": \"ط\",\n    \"Zah\": \"ظ\", \"Ain\": \"ع\", \"Ghain\": \"غ\", \"Feh\": \"ف\",\n    \"Qaf\": \"ق\", \"Kaf\": \"ك\", \"Lam\": \"ل\", \"Meem\": \"م\",\n    \"Noon\": \"ن\", \"Heh\": \"ه\", \"Waw\": \"و\", \"Yeh\": \"ي\",\n    \"Laa\": \"لا\", \"Teh_Marbuta\": \"ة\", \"Al\": \"ال\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:22:03.615059Z","iopub.execute_input":"2025-03-08T23:22:03.615734Z","iopub.status.idle":"2025-03-08T23:22:03.627603Z","shell.execute_reply.started":"2025-03-08T23:22:03.615705Z","shell.execute_reply":"2025-03-08T23:22:03.626772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_hand(image_path):\n    mp_hands = mp.solutions.hands\n    \n    # Read image\n    image = cv2.imread(image_path)\n    if image is None:\n        return 0  \n\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Suppress MediaPipe logs by redirecting stderr\n    with open(os.devnull, \"w\") as f, mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:\n        stderr_original = sys.stderr  # Save original stderr\n        sys.stderr = f  # Redirect stderr\n        results = hands.process(image_rgb)\n        sys.stderr = stderr_original  # Restore stderr\n    \n    return 1 if results.multi_hand_landmarks else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:22:03.628521Z","iopub.execute_input":"2025-03-08T23:22:03.628830Z","iopub.status.idle":"2025-03-08T23:22:03.639396Z","shell.execute_reply.started":"2025-03-08T23:22:03.628811Z","shell.execute_reply":"2025-03-08T23:22:03.638737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pd.DataFrame(columns=['Folder', 'Path', 'Hands'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:22:03.640300Z","iopub.execute_input":"2025-03-08T23:22:03.640546Z","iopub.status.idle":"2025-03-08T23:22:03.658517Z","shell.execute_reply.started":"2025-03-08T23:22:03.640515Z","shell.execute_reply":"2025-03-08T23:22:03.657850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\ndataset_path = \"/kaggle/input/rgb-arabic-alphabets-sign-language-dataset/RGB ArSL dataset\"\n\nfor folder in folders:\n    path = f\"{dataset_path}/{folder}\"  \n\n    # Get list of image files\n    image_files = !ls \"{path}\" | grep -E '\\.jpg$|\\.jpeg$|\\.png$|\\.JPG$|\\.JPEG$'\n    \n    if not image_files:\n        continue  \n    \n    total_files = len(image_files)\n\n    # Initialize progress bar with clean format\n    pbar = tqdm(total=total_files, desc=f\"{folder}: \", unit=\"\", bar_format=\"{desc}{percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt}\", ncols=50)\n    \n    for image_file in image_files:\n        image_path = f\"{path}/{image_file}\"\n        HandChecked = detect_hand(image_path)\n        new_row = pd.DataFrame([{'Folder': folder, 'Path': image_path, 'Hands': HandChecked}])\n        dataset = pd.concat([dataset, new_row], ignore_index=True)\n\n        pbar.update(1)  # Update progress bar\n    \n    pbar.close()\n\n","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":false,"execution":{"iopub.status.idle":"2025-03-08T23:39:35.997830Z","shell.execute_reply.started":"2025-03-08T23:22:03.660141Z","shell.execute_reply":"2025-03-08T23:39:35.996938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-08T23:56:04.269022Z","iopub.execute_input":"2025-03-08T23:56:04.269633Z","iopub.status.idle":"2025-03-08T23:56:04.289344Z","shell.execute_reply.started":"2025-03-08T23:56:04.269607Z","shell.execute_reply":"2025-03-08T23:56:04.288688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.to_csv(\"/kaggle/working/hand_detection_results.csv\", index=False)\nprint(\"✅ CSV file saved: /kaggle/working/hand_detection_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:54:28.594929Z","iopub.execute_input":"2025-03-08T23:54:28.595798Z","iopub.status.idle":"2025-03-08T23:54:28.626221Z","shell.execute_reply.started":"2025-03-08T23:54:28.595771Z","shell.execute_reply":"2025-03-08T23:54:28.625420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary_text = dataset.describe().to_string()  # Convert DataFrame summary to text\n\nwith open(\"/kaggle/working/dataset_summary.txt\", \"w\") as f:\n    f.write(summary_text)\n\nprint(\"✅ Dataset summary saved: /kaggle/working/dataset_summary.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T23:55:56.876381Z","iopub.execute_input":"2025-03-08T23:55:56.877114Z","iopub.status.idle":"2025-03-08T23:55:56.902431Z","shell.execute_reply.started":"2025-03-08T23:55:56.877077Z","shell.execute_reply":"2025-03-08T23:55:56.901742Z"}},"outputs":[],"execution_count":null}]}